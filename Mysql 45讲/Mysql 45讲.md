# 01 | 基础架构：查询的执行

![image-20200910112754326](Mysql 45讲.assets/image-20200910112754326.png)

MySQL可以分为**Server层**和**存储引擎**层两部分

Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务 功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

## 连接器   

### 内容

负责跟客户端**建立连接**、获取**权限**、**维持**和**管理**连接

户名密码认证通过，连接器会到权限表里面查出你**拥有的权限**。之后，这个连接里面的权限判断逻辑，都将依赖于此时**读**	权限。

一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也**不会**影响已经**存在连接的权限**。修改完成后，只有再新建的连接才会使用新的权限设置。

### show processlist：空闲连接

- 如果太长时间没动静，连接器就会**自动**将它**断开**,`wait_timeout`默认值是8小时
- 被断开之后，再次发送请求，收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要**重连**

长连接：是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。

短连接：则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个

### <font color=red>尽量使用长连接</font>

​	使用长连接可能会发现，**内存涨得特别快**，

​		MySQL在执行过程中临时使用的内存是**管理在连接对象**里面**的**。这些资源会在**连接断开**的时候**才释放**。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现 象看就是MySQL异常重启了。

**下两种方案**

1. 定期断开长连接
2. 可以在每次执行一个比较大的操作后，通过执行 `mysql_reset_connection`来重新初始化连接资源，这个过程**不需要**重连和重新做权限验证， 但是会将连接恢复到刚刚创建完时的状态

### 查询缓存

一个查询请求后，先到查询缓存。之前执行过 的语句及其结果可能会以key-value对的形式，被直接缓存在**内存**中。

key是查询的语句，value是 查询的结果。

**但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于 但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。** 

查询缓存的**失效**非常**频繁**(一条更新，全表缓存失效)

按需查缓存

```mysql
mysql> select SQL_CACHE * from T where ID=10；
```

## 分析器 

从你输入的"select"这个关键字识别出来，这是一个查询语句。

## 优化器

优化器是在表里面有多个索引的时候，**决定使用哪个索引**

或者在一个语句有多表关联（join） 的时候，**决定各个表的连接顺序**

## 执行器

先判断一下你对这个表T有没有执行查询的**权限**，如果有权限，就打开表继续执行。打开表的时候，执行器就会**根据表的引擎定义**，去使用这个引擎提供的**接口**。

## 问题?

如果表T中没有字段k，而你执行了这个语句 select * from T where k=1, 那 肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是 在我们上面提到的哪个阶段报出来的呢？

# 02 | 日志系统：更新语句是如何执行	

MySQL可以恢复到半个月内任意一秒的状态

在一个表上有更新的时候，跟这个表有关的查询缓存会失效，不建议使用查询缓存的原因。

与查询流程不一样的是，更新流程还涉及两个重要的日志模块

**redo log（重做日志）和 binlog（归档日志）**

##  重要的日志模块：redo log

![image-20200910141053475](Mysql 45讲.assets/image-20200910141053475.png)

粉板和账本配合的整个过程，其实就是MySQL里经常说到的**WAL技术**

InnoDB引擎就会先把记录写到redo log（粉板）里 面，并**更新内存**，这个时候更新就算完成了

![image-20200911174039536](Mysql 45讲.assets/image-20200911174039536.png)

write pos是当前**记录**的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。 

checkpoint是当前要**擦除**的位置，也是往后推移并且循环的，擦除记录前**要把记录更新到数据文件**。

有了redo log，InnoDB就可以保证即使数据库发生异常重启，**之前提交的记录都不会丢失**，这个 能力称为`crash-safe` 。

## 重要的日志模块：binlog

redo log是 InnoDB引擎特有的日志，而**Server层**也有自己的日志，称为binlog（归档日志）。

两种日志有以下三点不同

1. redo log是InnoDB引擎特有的；binlog是MySQL的**Server层**实现的，所有引擎都可以使用。
2. redo log是**物理日志**，记录的是“在某个数据页上做了什么修改”；binlog是**逻辑日志**，记录的 是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。

3. redo log是**循环写**的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件 写到一定大小后会切换到下一个，并不会覆盖以前的日志。


## update语 句时的内部流程

![image-20200911175032011](Mysql 45讲.assets/image-20200911175032011.png)

![image-20200911175103220](Mysql 45讲.assets/image-20200911175103220.png)

将redo log的写入拆成了两个步骤：prepare和 commit，这就是"两阶段提交"。

## 两阶段提交

binlog会记录所有的逻辑操作，并且是采用“**追加写**”的形式

系统会定期做整库备份。

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

- 首先，找到**最近的一次全量备份**，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库； 
- 然后，从备份的时间点开始，将**备份的binlog依次取出来**，重放到中午误删表之前的那个时 刻。

为什么日志需要“两阶段提交”？

![image-20200914161150206](Mysql 45讲.assets/image-20200914161150206.png)

可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的 **状态不一致**。

简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

## 小结

![image-20200914161725823](Mysql 45讲.assets/image-20200914161725823.png)

## 问题？

前面我说到定期全量备份的周期“取决于系统重要性，有 的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或 者说，它影响了这个数据库系统的哪个指标？

好处是“最长恢复时间”更短

系统的对应指标就是 @尼古拉斯·赵四 @慕塔 提到的RTO（恢复目标时间）。

更频繁全量备份需要消耗更多存储空间，所以这个RTO是成本换来 的，就需要你根据业务重要性来评估了。

# 03 | 事务隔离

## 隔离性与隔离级别

脏读（dirty read）、不可重复读（nonrepeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概 念。

![image-20200914162559371](Mysql 45讲.assets/image-20200914162559371.png)

![image-20200914162626063](Mysql 45讲.assets/image-20200914162626063.png)

![image-20200914162650885](Mysql 45讲.assets/image-20200914162650885.png)

数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。

1. 在“**可重复读**”隔离 级别下，这个视图是在**事务启动时**创建的，整个事务存在期间都用这个视图。
2. 在“**读提交**”隔离级 别下，这个视图是在每个**SQL语句开始执行的**时候创建的。这里需要注意的是，
3. “**读未提交**”隔离 级别下直接返回记录上的最新值，**没有视图概念**；而“串行化”隔离级别下直接用**加锁**的方式来避 免并行访问。

## 	事务隔离的实现 

​	实际上每条记录在**更新的时候都会同时记录一条回滚操作**

![image-20200914163302200](Mysql 45讲.assets/image-20200914163302200.png)

同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于read-view A，**要得到1**，就**必须**将当前 值**依次**执行图中所有的回滚操作得到。

什么时候删除呢？答案是，在不需要的时候才删除。

就是当系统里没有比这个回滚日志更早的read-view的时候

尽量不要使用长事务

- 导致大量占用存储空间。
- 长事务还占用锁资源，也可能拖垮整个库


## 事务的启动方式

set autocommit=0，自动提交关掉。

1. 随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。

2. 只读事务可以去掉

3.  SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。

   ![image-20200914170553881](Mysql 45讲.assets/image-20200914170553881.png)

## 问题

我给你留一个问题吧。你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也
是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

# 04 | 深入浅出索引

![image-20200914165758275](Mysql 45讲.assets/image-20200914165758275.png)

- 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为**聚簇索引**（clustered index）。
- 非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引 （secondary index）。

显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 .

## 索引维护

![image-20200914171157962](Mysql 45讲.assets/image-20200914171157962.png)

## 问题

![image-20200914170517033](Mysql 45讲.assets/image-20200914170517033.png)

![image-20200914171010726](Mysql 45讲.assets/image-20200914171010726.png)

重建索引k的做法是**合理**的，可以达到省空间的目的。但是，重建主键的过程**不合理**,都会将整个表重建。。

![image-20200914171457854](Mysql 45讲.assets/image-20200914171457854.png)

## SQL查询语句的执行流程：

![image-20200914171545683](Mysql 45讲.assets/image-20200914171545683.png)

## 覆盖索引	

由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用 由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段

## 最左前缀原则

字段顺序排序的

在建立联合索引的时候，如何安排索 在建立联合索引的时候，如何安排索引内的字段**顺序**

要考虑的原则就是**空间**

## 索引下推

![image-20200915160356972](Mysql 45讲.assets/image-20200915160356972.png)

![image-20200914172001135](Mysql 45讲.assets/image-20200914172001135.png)

![image-20200914172009340](Mysql 45讲.assets/image-20200914172009340.png)

而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索 引中包含的字段**先做判断**，直接过滤掉不满足条件的记录，减少回表次数

## 问题

![image-20200914172115388](Mysql 45讲.assets/image-20200914172115388.png)

![image-20200914172419813](Mysql 45讲.assets/image-20200914172419813.png)

![image-20200917163344491](Mysql 45讲.assets/image-20200917163344491.png)

![image-20200917163404203](Mysql 45讲.assets/image-20200917163404203.png)

# 06 | 全局锁和表锁 

根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁

## 全局锁 FTWRL

- 加全局读锁的方法，**Flush tables with read lock (FTWRL)。**


可以使用这个命令，之后其他线程的以下语句会被阻塞：数据**更新语句**（数据的增删改）、数据定义语句（包括
建表、**修改表结构**等）和更新类事务的提交语句。

- 还有mysqldump
- 还有set global readonly=true set global readonly=true

### 使用场景

全库逻辑备份

危险的地方：

- 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 
- 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟

如果不使用

![image-20200915161059437](Mysql 45讲.assets/image-20200915161059437.png)

备份结果里，用户A的数据状态是“账户余额没扣，但是用户课程表里面已经多了 一门课”。如果后面用这个备份来恢复数据的话，用户A就发现，自己赚了

结果：备份系统备份的得到的库**不是一个逻辑时间点**，这个视图是逻辑不一致的。

其实是有一个方法能够拿到一致性视图的：可重复读隔离级别下开启一个事务

### mysqldump

官方自带的逻辑备份工具是`mysqldump`（一致性读）

使用参数–single-transaction的时候，导数据**之前就会启动一个事务**，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是 可以正常更新的。

一致性读是好，但前提是引擎要支 一致性读是好，但前提是引擎要支 持这个隔离级别。 持这个**隔离级别**。比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是 只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。

全库只读，为什么不使用 既然要全库只读，为什么不使用**set global readonly=true**的方式呢 的方式呢？

### set global readonly=true

建议你用FTWRL方式。

- 一是，在有些系统中，readonly的值会被用来做其他逻辑，比如用来**判断一个库是主库还是备库**。因此，修改global变量的方式影响面更大，我不建议你使用。 
- 二是，在**异常处理机制**上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么 MySQL**会自动释放**这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly之后，如果客户端发生异常，则数据库就会**一直保持readonly状态**，这样会导致整个 库长时间处于不可写状态，风险较高。

## 表级锁 

一种是表锁，一种是元数据锁（meta data lock，MDL)。

### 表锁

表锁的语法是 表锁的语法是 `lock tables … read/write `。 与FTWRL类似，可以用unlock tables主动释放锁， 也可以在**客户端断开**的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了**本线程**接下来的操作对象。

线程A

执行lock tables t1 read, t2 write; <font color=red >相当于write 是个锁</font>。

//**其他线程**写t1、读 写t2的语句都会被阻塞，只能读 t1。

//线程A在执行unlock tables之前，也只能执行**读t1、读写t2**的操 作。连写t1都不允许，自然也不能访问其他表。

### MDL（metadata lock)

MDL不需要显式使用，在访问一个表的时候会被自动加上

作用: 

​	保证读写的正确性。

当对一个表做增删改查操作的时候，加**MDL读锁**；当 要对表做结构变更操作的时候，加**MDL写锁**。

- 读锁之间**不互斥**，因此你可以有多个线程同时对一张表增删改查。

- 读写锁之间、写锁之间是**互斥**的，用来保证变更**表结构**操作的**安全性**。因此，如果有两个线
  程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

> 事件：给一个小表加个字段，导致整个库挂了。
>

给一个表加字段，或者修改字段，或者加索引，需要**扫描全表**的数据

![image-20200917161952864](Mysql 45讲.assets/image-20200917161952864.png)

如果只有session C自己被阻塞还没什么关系，但是之后所有要在表t上新申请MDL读锁的请求也 会被session C阻塞。前面我们说了，所有对表的增删改查操作都需要先申请MDL读锁，就都被 锁住，**等于这个表现在完全不可读写了**

最终结果：如果某个表上的查询语句频繁，而且客户端有**重试机制**，也就是说超时后会再起一个新session 再请求的话，这个库的**线程很快就会爆满**。

事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释 放，**而会等到整个事务提交后再释放。**

> 如何安全地给小表加字段？
>

首先我们要解决长事务，事务不提交，就会一直占着MDL锁

information_schema 库的 innodb_trx 表中，你可以**查到当前执行中的事务**。

要考虑先**暂停**DDL，或者**kill**掉这个长事务。

如果是表是一个热点表，虽然数据量不大，但是上面的请求很**频繁**，而你不得不加个字段，你该怎么做？

![image-20200917162653261](Mysql 45讲.assets/image-20200917162653261.png)

## 问题

备份一般都会在备库上执行，你在用–single-transaction方法做逻 辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从 备库上会看到什么现象呢？

![image-20201109140945294](Mysql 45讲.assets/image-20201109140945294.png)

在备份开始的时候，为了确保RR（可重复读）隔离级别，再设置一次RR隔离级别(Q1); 

启动事务，这里用 WITH CONSISTENTSNAPSHOT确保这个语句执行完就可以得到一个一致性 视图（Q2)；

 设置一个保存点，这个很重要（Q3）；

 showcreate 是为了拿到表结构(Q4)，然后正式导数据 （Q5），回滚到SAVEPOINTsp，在这 里的作用是释放 t1的MDL锁 （Q6。当然这部分属于“超纲”，上文正文里面都没提到。

 DDL从主库传过来的时间按照效果不同，我打了四个时刻。题目设定为小表，我们假定到达后， 如果开始执行，则很快能够执行完成。

![image-20201109141138120](Mysql 45讲.assets/image-20201109141138120.png)

# 07 | 行锁功过：减少行锁对性能的影响

##  从两阶段锁说起

![image-20201109110515542](Mysql 45讲.assets/image-20201109110515542.png)

实际上事务B的update语句会被阻塞，直到事务A执行**commit**之后，事务B**才能继续**执行。 

**如果你的事务中需要锁多个行，要把 最可能造成锁冲突、最可能影响并发度的锁尽量往后放。**

比如：实现一个电影票在线交易业务

![image-20201109110822851](Mysql 45讲.assets/image-20201109110822851.png)

我们需要update两条记录，并insert一条记录

两个事务冲突的部分就是语句2了

根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句2安排在最后，比如按照3、1、2这样的顺序，那么影院账户余额 **这一行的锁时间就最少**。这就最大程度地减少了事务之间的锁等待，提升了并发度。

## 死锁和死锁检测

当出现死锁以后，有两种策略：

- 一种策略是，直接进入等待，直到**超时**。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。 
- 另一种策略是，**发起死锁检测**，发现死锁后，**主动回滚**死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。 

在InnoDB中，innodb_lock_wait_timeout的默认值是50s，

正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect的默认值本身就是on。主动死锁检测在发生锁的时候，是能够快速发 现并进行处理的，但是它也是有额外负担的。 

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级 的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到 **CPU利用率很高，但是每秒却执行不了几个事务**。

## 怎么解决由这种热点行更新导致的性能问题呢？

- **一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检 测关掉**意味着可能会出现大量的超时，这是业务有损的
- **另一个思路是控制并发度。**你会很快发现这个方法不太可行，因为客户端很多。我见过 一个应用，有600个客户端，这样即使每个客户端控制到只有5个并发线程，汇总到数据库服务 端以后，峰值并发数也可能要达到3000。
- 因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的 团队有能修改MySQL源码的人，也可以做在MySQL里面。基本思路就是，对于相同行的更新， 在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了

没有数据库方面的专家，怎么样优化这个问题呢？

你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多 条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账 户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等 待个数，也就减少了死锁检测的CPU消耗。 这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会 减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成0的时候，代码要有特殊处 理。

## 小结

涉及了两阶段锁协议、死锁和死锁检测这两大部分内容

我给你的建议是：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并 发度的锁的申请时机尽量往后放

但是，调整语句顺序并不能完全避免死锁。所以我们引入了死锁和死锁检测的概念，以及提供了 三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是**控制访问相同资源的并发事** 务量。

## 问题

如果你要删除一个表里面的前10000行数据

- 第一种，直接执行delete fromTlimit 10000; 

- 第二种，在一个连接中循环执行20次 delete fromTlimit 500;

-  第三种，在20个连接中同时执行delete fromTlimit 500。

  

  ​	第二种方式是相对较好的即：在一个连接中循环执行20次 delete fromTlimit 500。 确实是这样的。

  第一种方式（即：直接执行delete fromTlimit 10000）里面，单个语句占用时间长，锁的时间也 比较长；而且大事务还会导致主从延迟。

  第三种方式（即：在20个连接中同时执行delete fromTlimit 500），会人为造成锁冲突。

# 08 | 事务到底是隔离的还是不隔离的？

```mysql
mysql> CREATE TABLE `t` ( 
    `id` int(11) NOT NULL, 
    `k` int(11) DEFAULT NULL, 
    PRIMARY KEY (`id`)
) ENGINE=InnoDB; 

insert into t(id, k) values(1,1),(2,2);

```

![image-20201109141910584](Mysql 45讲.assets/image-20201109141910584.png)

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表 的语句，事务才真正启动。如果你想要**马上启动一个事务**，可以使用start transaction with consistent snapshot 这个命令。

还需要注意的是，在整个专栏里面，我们的例子中如果没有特别说明，都是默认 autocommit=1。（自动提交）

事务C没有显式地使用begin/commit，表示这个update语句本身就是一个事务， 语句完成的时候会**自动提交**。

结果：，如果我告诉你事务B查到的k的值是3，而事务A查到的k的值是1。

在MySQL里，有两个“视图”的概念：

- 一个是view。
- 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持 RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。

## “快照”在MVCC里是怎么工作的？

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个**快照是基于整库**的。

InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向 InnoDB的事务系统申请的，是按申请顺序严格递增的。

数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的rowtrx_id。

一个记录被多个事务连续更新后的状态。

![image-20201109143705661](Mysql 45讲.assets/image-20201109143705661.png)

实际上，图2中的三个虚线箭头，就是undo log；

而V1、V2、V3并**不是物理上真实存在的**，而 是每次需要的时候根据当前版本和undo log**计算**出来的。比如，需要V2的时候，就是通过V4依 次执行U3、U2算出来。

InnoDB是怎么定义那个“100G”的快照 的。

按照可重复读的定义，一个事务启动的时候，能够看到所有**已经提交**的事务结果。但是之后，这 个事务执行期间，其他事务的更新对它不可见。

在实现上， InnoDB为**每个事务构造了一个数组**，用来保存这个事务启动瞬间，当前正在“活 跃”的所有事务ID。“活跃”指的就是，**启动了但还没提交。**

![image-20201109145243146](Mysql 45讲.assets/image-20201109145243146.png)

![image-20201109145314029](Mysql 45讲.assets/image-20201109145314029.png)

InnoDB利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建 快照”的能力。

![image-20201109145614796](Mysql 45讲.assets/image-20201109145614796.png)

![image-20201109145639484](Mysql 45讲.assets/image-20201109145639484.png)

你可能注意到了，在事务A查询的时候，其实事务B还**没有提交**，但是它生成的(1,3)这个版本已 经变成当前版本了。但这个版本对事务A必须是不可见的，否则就变成脏读了。

![image-20201109150630959](Mysql 45讲.assets/image-20201109150630959.png)

## 更新逻辑

![image-20201109150737552](Mysql 45讲.assets/image-20201109150737552.png)

你看图5中，事务B的视图数组是先生成的，之后事务C才提交，不是应该看不见(1,2)吗，怎么能 算出(1,3)来？

是的，如果事务B在更新之前查询一次数据，这个查询返回的k的值确实是1。

**更新数据都是先读后写的，而这个读，只能读当前的 值，称为“当前读”（current read）。**

所以，在执行事务B查询语句的时候，一看自己的版本号是101，最新数据的版本号也是101，是 自己的更新，可以直接使用，所以查询得到的k的值是3。

![image-20201109151028224](Mysql 45讲.assets/image-20201109151028224.png)

我们在上一篇文章中提到的“**两阶段锁协议**”就要上场了。事务C’没提交，也就是说(1,2) 这个版本上的**写锁还没释放**。而事务B是**当前读，必须要读最新版本**，而且必须加锁，因此就被 锁住了，必须等到事务C’释放这个锁，才能继续它的当前读。

![image-20201109152306677](Mysql 45讲.assets/image-20201109152306677.png)

可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如 果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

- 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询 都共用这个一致性视图；
-  在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

这里需要说明一下，“start transaction with consistent snapshot; ”的意思是从这个语句开始，创 建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于 普通的start transaction。

## 下面是读提交时的状态图

![image-20201109152521076](Mysql 45讲.assets/image-20201109152521076.png)

![image-20201109152607846](Mysql 45讲.assets/image-20201109152607846.png)

## 小结

InnoDB的行数据有多个版本，每个数据版本有自己的rowtrx_id，每个事务或者语句有自己的一 致性视图。普通查询语句是一致性读，一致性读会根据rowtrx_id和一致性视图确定数据版本的 可见性。

-  对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
-  对于读提交，查询只承认在语句启动前就已经提交完成的数据；

这是因为表结构没有对应的行数据，也没有 rowtrx_id，因此只能遵循当前读的逻辑。

MySQL 8.0已经可以把表结构放在InnoDB字典里了，也许以后会支持表结构的可重复 读。

## 问题

![image-20201109154403032](Mysql 45讲.assets/image-20201109154403032.png)

答案：

![image-20201110154512452](Mysql 45讲.assets/image-20201110154512452.png)

![image-20201110154531228](Mysql 45讲.assets/image-20201110154531228.png)

# 09 | 普通索引和唯一索引，应该怎么选择？

假设你在维护一个市民系统，每个人都有一个**唯一的身份证号**，而且业务代码已经保证了不会写 入两个重复的身份证号。如果市民系统需要按照身份证号查姓名，就会执行类似这样的SQL语 句：

```mysql
select name from CUser where id_card = 'xxxxxxxyyyyyyzzzzz';
```

一定会考虑在id_card字段上建索引。

身份证号字段比较大，我不建议你把身份证号当做主键

那么现在你有两个选择，要么给 id_card字段创建唯一索引，要么创建一个普通索引。如

我们就从这两种索引对查询语句和更新语句的性能影响来进行分析

## 查询过程

select id fromTwhere k=5。可以认为数据页内部通过二分法来定位记录

- 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰 **到第一个不满足**k=5条件的记录。
-  对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。

性能差距微乎其微。

InnoDB的数据是按**数据页**为单位来读写的。当找到k=5的记录的时候，它所在的数据页就都在**内存**里了。要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针 寻找和一次计算。

当然，如果k=5这个记录刚好是这个数据页的**最后一个记录**，那么要取下一个记录，必须读取下 一个数据页，这个操作会稍微复杂一些。

对于整型字段，一个数据页可以放近千个key，因此出现这种情况的概 率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的CPU来说可以 **忽略不计**。

## 更新过程（change buffer）

![image-20201109162144269](Mysql 45讲.assets/image-20201109162144269.png)

当需要更新一个数据页时，如果**数据页在内存中**就直接更新，而如果这个数据页还没有在内存中 的话，在不影响**数据一致性**的前提下，InooDB会将这些更新操作**缓存在change buffer**中，这样 就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将**数据页读入内 存**，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正 确性。

将change buffer中的操作**应用到原数据页**，得到最新结果的过程称为`merge`。除了**访问这个数据 页**会触发merge外，系统有后台线程会**定期merge**。在数据库**正常关闭**（shutdown）的过程中， 也会执行merge操作。数据读入内存是需要占用**buffer pool**的，所以这种方式还能够避免占用内存，提高内存利用率。

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400)这个记录，就要先判断现在表中是否已经存在k=4的记录，而这必须要将数据页读入内存 才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer 了。

 因此，唯一索引的更新就**不能**使用change buffer，实际上也**只有普通索引**可以使用。

![image-20201109170208165](Mysql 45讲.assets/image-20201109170208165.png)

change buffer用的是buffer pool里的内存,change buffer的大小，可以通 过参数innodb_change_buffer_max_size来**动态设置**。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。

### 如果要在这张表中插入一个 新记录(4,400)的话，InnoDB的处理流程是怎样的

第一种情况是，这个记录要更新的目标页**在内存中**

- 对于唯一索引来说，找到3和5之间的位置，判断到**没有冲突**，插入这个值，语句执行结束；

-  对于普通索引来说，找到3和5之间的位置，插入这个值，语句执行结束。

  差别，只是一个判断，只会耗费微小的 CPU时间

第二种情况是，这个记录要更新的目标页**不在内存中**。

- 对于唯一索引来说，需要将数据页**读入内存**，判断到没有冲突，**插入这个值**，语句执行结束； 

- 对于普通索引来说，则是将更新记录在**change buffer**，语句执行就结束了。

  从磁盘读入内存涉及**随机IO**的访问，是数据库里面成本最高的操作之一。change buffer 因为**减少了**随机磁盘访问，所以对更新性能的提升是会很明显的。

## change buffer的使用场景

普通 索引的所有场景，使用change buffer都可以起到加速作用吗？

所以在一个数据页做merge之前，change buffer记录的变更越多（也就是这个页面 上要更新的次数越多），收益就越大。

对于**写多读少**的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。

将更新先记 录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问 IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来 说，change buffer反而起到了副作用。

## 索引选择和实践

这两类索引在查询能力上 是没差别的，主要考虑的是对**更新性能**的影响。所以，我建议你尽量选择普通索引。

如果所有的**更新后面，都马上伴随着对这个记录的查询**，那么你应该`关闭change buffer`。而在 其他情况下，change buffer都能提升更新性能。

## change buffer 和 redo log

WAL 提升性能的核心机 制，也的确是尽量减少**随机读写**

k1 数据页再内存

k2 不在

![image-20201110153240140](Mysql 45讲.assets/image-20201110153240140.png)

![image-20201110153316985](Mysql 45讲.assets/image-20201110153316985.png)

然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。

我们现在要执行 select *fromt where k in (k1, k2)。

![image-20201110153519192](Mysql 45讲.assets/image-20201110153519192.png)

![image-20201110153555894](Mysql 45讲.assets/image-20201110153555894.png)

**redo log 主要节省的是随 机<font color="red">写</font>磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机<font color="red">读</font>磁盘的IO消 耗。**

## 小结

普通索引和唯一索引的选择开始，和你分享了数据的查询和更新过程，然后说明了 change buffer的机制以及应用场景，最后讲到了索引选择的实践。

唯一索引**用不上**change buffer的优化机制

如果碰上了大量插入数据慢、内存 命中率低的时候，可以给你多提供一个排查思路

## 问题

change buffer一开始是写内存的，那么如果这个时候机器掉电重启，会不 会导致change buffer丢失呢？change buffer丢失可不是小事儿，再从磁盘读入数据可就没有了 merge过程，就等于是数据丢失了。会不会出现这种情况呢？

答案是不会丢失，留言区的很多同学都回答对了。虽然是只更新内存，但是在事务提交的时候，我们把change buffer的操作也记录到**redo log**里了，所以崩溃恢复的时候，change buffer也能找回来。

![image-20201110165513483](Mysql 45讲.assets/image-20201110165513483.png)

# 10 | MySQL为什么有时候会选错索引？analyze table t

```mysql
CREATE TABLE `t` ( 
    `id` int(11) NOT NULL,
    `a` int(11) DEFAULT NULL,
    `b` int(11) DEFAULT NULL,
    PRIMARY KEY (`id`), 
    KEY `a` (`a`), KEY `b` (`b`) 
) ENGINE=InnoDB；
 ----往表t中插入10万行记录
delimiter ;;
create procedure idata()
begin
declare i int;
set i=1;
while(i<=100000)do
insert into t values(i, i, i);
set i=i+1;
end while;
end;;
delimiter ;
call idata();
```

## 情况一：

```

mysql> select * from t where a between 10000 and 20000;
```

你会说：a上有索引，肯定是要使用索引a的

![image-20201110154953239](Mysql 45讲.assets/image-20201110154953239.png)

也确实符合预期

![image-20201110155028131](Mysql 45讲.assets/image-20201110155028131.png)

session A：开启了一个事务

session B：把数据都删 除后，又调用了 idata这个存储过程，插入了10万行数据

这时候，session B的查询语句select *fromt where a between 10000 and 20000就**不会再选择索引a**了。我们可以通过慢查询日志（slowlog）来查看一下具体的执行情况。

![image-20201110155211672](Mysql 45讲.assets/image-20201110155211672.png)

![image-20201110155231304](Mysql 45讲.assets/image-20201110155231304.png)

Q1扫描了10万行

Q2扫描了10001行

## 优化器的逻辑

选择索引是优化器的工作

是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库 里面，**扫描行数**是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越 少，消耗的CPU资源越少。

### 扫描行数是怎么判断的？

真正开始执行语句之前，并**不能精确**地知道满足这个条件的记录有多少条，而只能根 据统计信息来估算记录数。

这个统计信息就是索引的**“区分度**”。显然，一个索引上不同的值越多，这个索引的区分度就越 好。而一个索引上**不同**的值的个数，我们称之为“**基数**”（cardinality）。也就是说，这个基数越大，索引的区分度越好。

show index方法，看到一个索引的基数

![image-20201110155748271](Mysql 45讲.assets/image-20201110155748271.png)

MySQL是怎样得到索引的基数的呢？

#### 采样统计

InnoDB默认会选择N个**数据页**，统计这些页面上的**不同值**，得到一个**平均值**，然后乘以这个**索引的页面数**，就得到了这个索引的**基数**。

而数据表是会**持续更新**的，索引统计信息也不会固定不变。所以，当**变更**的数据行数**超过1/M**的 时候，会自动触发**重新**做一次**索引统计**。

![image-20201110160226554](Mysql 45讲.assets/image-20201110160226554.png)

rows这个字段表示的是预计扫描行数。

而是优化器为什么放着扫描37000行的执行计划 不用，却选择了扫描行数是100000的执行计划呢？

如果使用索引a，每次从索引a上拿到一个值，都要回到主键索引上查出整行数据， 这个代价优化器也要算进去的。(**回表**)

至于 为什么会得到错误的扫描行数，这个原因就作为课后问题，留给你去分析了，

`analyze table t 来修正统计信息`

## 情况二

```
mysql> select * from t where (a between 1 and 1000) and (b between 50000 and 100000) 
```

![image-20201110163239036](Mysql 45讲.assets/image-20201110163239036.png)

![image-20201110163302612](Mysql 45讲.assets/image-20201110163302612.png)

这次优化器选择了索引b，而rows字段显示需要扫描的行 数是50198。 

之前优化器选择使用索引b，是因为它认为使用索引b可以**避免排序**（b本身是索引，已经是有序 的了，如果选择索引b的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为 代价更小。

从这个结果中，你可以得到两个结论： 

1. 扫描行数的估计值依然不准确；
2.  这个例子里MySQL又选错了索引。

## 解决方案

- 一种方法是，像我们第一个例子一样，采用`force index`强行选择一个索引。

缺点：一来这么写不优美，二来如果索引改了名字，这个语句 mysql> explain select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1; 也得改，显得很麻烦。而且如果以后迁移到别的数据库的话，这个语法还可能会不兼容

数据库的问题最好还是在数据库内部来解决

- 第二种方法就是，我们可以考虑修改 语句，引导MySQL使用我们期望的索引。

order byb limit 1” 改 成 “order byb,a limit 1” ，语义的逻辑是相同的。

按照b,a排序，就意味着使用这两个索引**都需要排序**

- 第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选 择，或**删掉误用的索引**。


## 小结

聊索引统计的更新机制，优化器存在选错索引的可能性（基数，回表，排序）

`analyze table来解决`

force index来强行指定索引，也可以通过修改 语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。

## 问题

过session A的配合， 让session B删除数据后又重新插入了一遍数据，然后就发现explain结果中，rows字段从10001 变成37000多。

而如果没有session A的配合，只是单独执行delete fromt 、call idata()、explain这三句话，会看 到rows字段其实还是10000左右。你可以自己验证一下这个结果。为什么呢？



但是，session A开启了事务并没有提交，所以之前插入的10万行数据是不能删除的。这样，之 前的数据每一行数据都有**两个版本**，旧版本是delete之前的数据，新版本是标记为deleted的数 据。

这样，索引a上的数据其实就有两份。 然后你会说，不对啊，主键上的数据也不能删，那没有使用force index的语句，使用explain命令 看到的扫描行数为什么还是100000左右？（潜台词，如果这个也翻倍，也许优化器还会认为选 字段a作为索引更合适）

 是的，不过这个是主键，主键是直接按照表的行数来估计的。而表的行数，优化器直接用的是 **showt able status**的值。

# 11 | 怎么给字符串字段加索引？

邮箱这样的字段上建立合理的索引

![image-20201110170424169](Mysql 45讲.assets/image-20201110170424169.png)

## 前缀索引

```mysql
alter table SUser add index index2(email(6));
```

![image-20201110170719851](Mysql 45讲.assets/image-20201110170719851.png)

**使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查 询成本。**

我们要看一下4~7个字节的前缀索引

![image-20201110170918280](Mysql 45讲.assets/image-20201110170918280.png)

### 前缀索引对覆盖索引的影响

如果使用index1（即email整个字符串的索引结构）的话，可以利用覆盖索引，从index1查 到结果后直接就返回了，不需要回到ID索引再去查一次。而如果使用index2（即email(6)索引结 构）的话，就不得不回到ID索引再去判断email字段的值。

即使你将index2的定义修改为email(18)的前缀索引，要回到id索引再查一下，定义是否截断了完整信 息。

## 其他方式

假设你维护的数据库是一个市的公民信息系统，这时候如果对身份证号做长度为6的前缀索引的 话，这个索引的区分度就非常低了。

### 第一种方式是使用倒序存储。

```mysql
mysql> select field_list from t where id_card = reverse('input_id_card_string');
```

所以最后这6位很可能就提供了足够的区 分度。当然了，实践中你不要忘记使用count(distinct)方法去做个验证

### 第二种方式是使用hash字段

```mysql
mysql> alter table t add id_card_crc int unsigned, add index(id_card_crc);
```

每次插入新记录的时候，都同时用**crc32()**这个函数得到校验码填到这个新字段，两个不同的身份证号通过crc32()函数得到的结果可能是相同的，where部分要判断id_card的值是否精确相同。

索引的长度变成了4个字节，比原来小了很多。

1. 从占用的**额外空间**来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字 段方法需要增加一个字段。当然，倒序存储方式使用4个字节的前缀长度应该是不够的，如 果再长一点，这个消耗跟额外这个hash字段也**差不多抵消**了。
2. 在**CPU消耗**方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash 字段的方式需要额外**调用一次**crc32()函数。如果只从这两个函数的**计算复杂度**来看的 话，**reverse函数**额外消耗的CPU资源会更小些。
3. 从查询效率上看，使用**hash**字段方式的查询性能相对更稳定一些。因为crc32算出来的值虽 然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近1。而倒序存储 方式毕竟还是用的**前缀索引**的方式，也就是说还是会增加扫描行数。

## 小结

![image-20201110172225966](Mysql 45讲.assets/image-20201110172225966.png)

## 问题



































































































































































